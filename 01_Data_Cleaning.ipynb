{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Cleaning and Preprocessing\n",
    "\n",
    "**Project**: House Price Prediction and Analysis Using King County Housing Data\n",
    "\n",
    "**Team**: Ashwin, Ashwath, Namrata Mane\n",
    "\n",
    "**Course**: DA 591 - Final Semester Project\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will clean and prepare the King County housing dataset for analysis. The steps include:\n",
    "1. Loading the data\n",
    "2. Understanding the data structure\n",
    "3. Checking for missing values\n",
    "4. Handling duplicates\n",
    "5. Converting data types\n",
    "6. Handling outliers\n",
    "7. Feature engineering\n",
    "8. Saving the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We will use pandas for data manipulation and numpy for numerical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Dataset\n",
    "\n",
    "Loading the King County housing dataset from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('kc_house_data.csv')\n",
    "\n",
    "# Check how many rows and columns we have\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: First Look at the Data\n",
    "\n",
    "Let's see what the data looks like - first few rows and the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last 5 rows to make sure data is complete\n",
    "print(\"Last 5 rows of the dataset:\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all column names\n",
    "print(\"Column names in the dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Understanding Data Types\n",
    "\n",
    "Let's check the data type of each column and see if any conversions are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of all columns\n",
    "print(\"Data types of each column:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more detailed info about the dataset\n",
    "print(\"Detailed information about the dataset:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Statistical Summary\n",
    "\n",
    "Let's look at the basic statistics of numerical columns to understand the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical columns\n",
    "print(\"Statistical summary of the dataset:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the price column specifically since it's our target variable\n",
    "print(\"Price column statistics:\")\n",
    "print(f\"Minimum price: ${df['price'].min():,.2f}\")\n",
    "print(f\"Maximum price: ${df['price'].max():,.2f}\")\n",
    "print(f\"Average price: ${df['price'].mean():,.2f}\")\n",
    "print(f\"Median price: ${df['price'].median():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Check for Missing Values\n",
    "\n",
    "Missing values can affect our analysis and model performance. Let's check if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "print(\"Missing values in each column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values in the entire dataset\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values in the dataset: {total_missing}\")\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"Great! No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(\"We need to handle these missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Check for Duplicate Records\n",
    "\n",
    "Duplicate records can skew our analysis. Let's check if any house is listed more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows (entire row is same)\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate house IDs\n",
    "# Same house might be sold multiple times\n",
    "duplicate_ids = df['id'].duplicated().sum()\n",
    "print(f\"Number of duplicate house IDs: {duplicate_ids}\")\n",
    "\n",
    "if duplicate_ids > 0:\n",
    "    print(f\"\\nThis means {duplicate_ids} houses were sold more than once.\")\n",
    "    print(\"We will keep only the latest sale for each house.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some duplicate IDs to understand\n",
    "if duplicate_ids > 0:\n",
    "    # Find houses that appear more than once\n",
    "    duplicate_houses = df[df['id'].duplicated(keep=False)]\n",
    "    print(f\"Total records for houses sold multiple times: {len(duplicate_houses)}\")\n",
    "    \n",
    "    # Show example of a house sold multiple times\n",
    "    sample_id = duplicate_houses['id'].iloc[0]\n",
    "    print(f\"\\nExample: House ID {sample_id}\")\n",
    "    print(df[df['id'] == sample_id][['id', 'date', 'price']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Handle Duplicates\n",
    "\n",
    "For houses sold multiple times, we will keep only the most recent sale since it reflects the current market value better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original count\n",
    "original_count = len(df)\n",
    "print(f\"Original number of records: {original_count}\")\n",
    "\n",
    "# Sort by date in descending order so latest sale comes first\n",
    "df_sorted = df.sort_values('date', ascending=False)\n",
    "\n",
    "# Keep only the first occurrence (latest sale) for each house ID\n",
    "df_cleaned = df_sorted.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# Reset the index\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of records after removing duplicates: {len(df_cleaned)}\")\n",
    "print(f\"Records removed: {original_count - len(df_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no more duplicate IDs\n",
    "remaining_duplicates = df_cleaned['id'].duplicated().sum()\n",
    "print(f\"Remaining duplicate IDs: {remaining_duplicates}\")\n",
    "\n",
    "if remaining_duplicates == 0:\n",
    "    print(\"All duplicates have been removed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Convert Date Column\n",
    "\n",
    "The date column is currently stored as a string. Let's convert it to a proper datetime format and extract useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current format of the date column\n",
    "print(\"Sample date values:\")\n",
    "print(df_cleaned['date'].head())\n",
    "print(f\"\\nCurrent data type: {df_cleaned['date'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "# The format is like '20141013T000000'\n",
    "df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], format='%Y%m%dT%H%M%S')\n",
    "\n",
    "print(\"Date column converted to datetime format:\")\n",
    "print(df_cleaned['date'].head())\n",
    "print(f\"\\nNew data type: {df_cleaned['date'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract useful features from the date\n",
    "df_cleaned['sale_year'] = df_cleaned['date'].dt.year\n",
    "df_cleaned['sale_month'] = df_cleaned['date'].dt.month\n",
    "\n",
    "print(\"New date features created:\")\n",
    "print(df_cleaned[['date', 'sale_year', 'sale_month']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the date range in our dataset\n",
    "print(f\"Date range of the data:\")\n",
    "print(f\"Earliest sale: {df_cleaned['date'].min()}\")\n",
    "print(f\"Latest sale: {df_cleaned['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Check for Unusual Values\n",
    "\n",
    "Let's check if there are any unusual or incorrect values in the data that don't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check bedrooms - can a house have 0 or very high number of bedrooms?\n",
    "print(\"Bedroom distribution:\")\n",
    "print(f\"Minimum bedrooms: {df_cleaned['bedrooms'].min()}\")\n",
    "print(f\"Maximum bedrooms: {df_cleaned['bedrooms'].max()}\")\n",
    "print(f\"\\nValue counts:\")\n",
    "print(df_cleaned['bedrooms'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Houses with 0 bedrooms - might be studios or data errors\n",
    "zero_bedrooms = df_cleaned[df_cleaned['bedrooms'] == 0]\n",
    "print(f\"Number of houses with 0 bedrooms: {len(zero_bedrooms)}\")\n",
    "\n",
    "if len(zero_bedrooms) > 0:\n",
    "    print(\"\\nThese might be studios or data errors. Let's look at them:\")\n",
    "    print(zero_bedrooms[['id', 'bedrooms', 'bathrooms', 'sqft_living', 'price']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Houses with very high bedrooms (more than 10)\n",
    "high_bedrooms = df_cleaned[df_cleaned['bedrooms'] > 10]\n",
    "print(f\"Number of houses with more than 10 bedrooms: {len(high_bedrooms)}\")\n",
    "\n",
    "if len(high_bedrooms) > 0:\n",
    "    print(\"\\nThese are unusual. Let's examine them:\")\n",
    "    print(high_bedrooms[['id', 'bedrooms', 'bathrooms', 'sqft_living', 'price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The house with 33 bedrooms seems like a data entry error\n",
    "# 33 bedrooms with only 1620 sqft is impossible\n",
    "# Let's check the sqft per bedroom ratio\n",
    "\n",
    "if len(high_bedrooms) > 0:\n",
    "    print(\"Checking sqft per bedroom for unusual entries:\")\n",
    "    for idx, row in high_bedrooms.iterrows():\n",
    "        sqft_per_bedroom = row['sqft_living'] / row['bedrooms']\n",
    "        print(f\"House ID {row['id']}: {row['bedrooms']} bedrooms, {row['sqft_living']} sqft\")\n",
    "        print(f\"  -> {sqft_per_bedroom:.2f} sqft per bedroom (should be at least 100)\")\n",
    "        \n",
    "        if sqft_per_bedroom < 80:\n",
    "            print(\"  -> This looks like a DATA ERROR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the obvious data error - house with 33 bedrooms should probably be 3\n",
    "# Looking at the data: 33 bedrooms, 1.75 bathrooms, 1620 sqft -> clearly an error\n",
    "\n",
    "error_mask = (df_cleaned['bedrooms'] == 33) & (df_cleaned['sqft_living'] < 2000)\n",
    "if error_mask.sum() > 0:\n",
    "    print(\"Fixing data entry error: 33 bedrooms -> 3 bedrooms\")\n",
    "    df_cleaned.loc[error_mask, 'bedrooms'] = 3\n",
    "    print(\"Fixed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for houses with 0 bathrooms\n",
    "zero_bathrooms = df_cleaned[df_cleaned['bathrooms'] == 0]\n",
    "print(f\"Number of houses with 0 bathrooms: {len(zero_bathrooms)}\")\n",
    "\n",
    "if len(zero_bathrooms) > 0:\n",
    "    print(\"\\n0 bathrooms is unusual for a house. Let's see:\")\n",
    "    print(zero_bathrooms[['id', 'bedrooms', 'bathrooms', 'sqft_living', 'price']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sqft_living - should not be 0 or negative\n",
    "print(\"Sqft_living check:\")\n",
    "print(f\"Minimum: {df_cleaned['sqft_living'].min()}\")\n",
    "print(f\"Maximum: {df_cleaned['sqft_living'].max()}\")\n",
    "\n",
    "if df_cleaned['sqft_living'].min() <= 0:\n",
    "    print(\"\\nWARNING: Found houses with 0 or negative living space!\")\n",
    "else:\n",
    "    print(\"\\nAll houses have valid living space values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Handle Outliers in Price\n",
    "\n",
    "Extreme outliers can affect our model performance. Let's identify and handle them using the IQR (Interquartile Range) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR for price\n",
    "Q1 = df_cleaned['price'].quantile(0.25)\n",
    "Q3 = df_cleaned['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"Price distribution:\")\n",
    "print(f\"Q1 (25th percentile): ${Q1:,.2f}\")\n",
    "print(f\"Q3 (75th percentile): ${Q3:,.2f}\")\n",
    "print(f\"IQR: ${IQR:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"\\nOutlier boundaries:\")\n",
    "print(f\"Lower bound: ${lower_bound:,.2f}\")\n",
    "print(f\"Upper bound: ${upper_bound:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count outliers\n",
    "price_outliers = df_cleaned[(df_cleaned['price'] < lower_bound) | (df_cleaned['price'] > upper_bound)]\n",
    "print(f\"Number of price outliers: {len(price_outliers)}\")\n",
    "print(f\"Percentage of data: {(len(price_outliers)/len(df_cleaned))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the distribution of outliers\n",
    "low_outliers = df_cleaned[df_cleaned['price'] < lower_bound]\n",
    "high_outliers = df_cleaned[df_cleaned['price'] > upper_bound]\n",
    "\n",
    "print(f\"Houses below lower bound (< ${lower_bound:,.0f}): {len(low_outliers)}\")\n",
    "print(f\"Houses above upper bound (> ${upper_bound:,.0f}): {len(high_outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this analysis, we will keep most of the data\n",
    "# Only remove extreme outliers (prices above 3 million or below 50k)\n",
    "# This is a practical decision since luxury homes are still valid data points\n",
    "\n",
    "# Let's see how many extreme outliers we have\n",
    "extreme_high = df_cleaned[df_cleaned['price'] > 3000000]\n",
    "extreme_low = df_cleaned[df_cleaned['price'] < 50000]\n",
    "\n",
    "print(f\"Extreme outliers:\")\n",
    "print(f\"Price > $3,000,000: {len(extreme_high)}\")\n",
    "print(f\"Price < $50,000: {len(extreme_low)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision: We will keep all data for now\n",
    "# Extreme luxury homes are still valid - they represent luxury market\n",
    "# Very cheap homes might be land or special cases\n",
    "\n",
    "print(\"Decision: Keeping all price data for analysis.\")\n",
    "print(\"Reason: Extreme values represent real market segments (luxury homes, land, etc.)\")\n",
    "print(\"\\nNote: If model performance is poor, we can revisit this decision.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Feature Engineering\n",
    "\n",
    "Let's create new features that might be useful for our analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'house_age' feature\n",
    "# Using the sale year to calculate how old the house was when sold\n",
    "df_cleaned['house_age'] = df_cleaned['sale_year'] - df_cleaned['yr_built']\n",
    "\n",
    "print(\"House age feature created:\")\n",
    "print(f\"Youngest house age at sale: {df_cleaned['house_age'].min()} years\")\n",
    "print(f\"Oldest house age at sale: {df_cleaned['house_age'].max()} years\")\n",
    "print(f\"Average house age at sale: {df_cleaned['house_age'].mean():.1f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'renovated' binary feature\n",
    "# 1 if the house was ever renovated, 0 otherwise\n",
    "df_cleaned['renovated'] = (df_cleaned['yr_renovated'] > 0).astype(int)\n",
    "\n",
    "print(\"Renovated feature created:\")\n",
    "print(df_cleaned['renovated'].value_counts())\n",
    "print(f\"\\nPercentage of renovated houses: {df_cleaned['renovated'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'price_per_sqft' feature\n",
    "df_cleaned['price_per_sqft'] = df_cleaned['price'] / df_cleaned['sqft_living']\n",
    "\n",
    "print(\"Price per sqft feature created:\")\n",
    "print(f\"Minimum: ${df_cleaned['price_per_sqft'].min():.2f}/sqft\")\n",
    "print(f\"Maximum: ${df_cleaned['price_per_sqft'].max():.2f}/sqft\")\n",
    "print(f\"Average: ${df_cleaned['price_per_sqft'].mean():.2f}/sqft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'has_basement' binary feature\n",
    "df_cleaned['has_basement'] = (df_cleaned['sqft_basement'] > 0).astype(int)\n",
    "\n",
    "print(\"Has basement feature created:\")\n",
    "print(df_cleaned['has_basement'].value_counts())\n",
    "print(f\"\\nPercentage of houses with basement: {df_cleaned['has_basement'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'total_rooms' feature (bedrooms + bathrooms gives a rough idea)\n",
    "df_cleaned['total_rooms'] = df_cleaned['bedrooms'] + df_cleaned['bathrooms']\n",
    "\n",
    "print(\"Total rooms feature created:\")\n",
    "print(f\"Minimum total rooms: {df_cleaned['total_rooms'].min()}\")\n",
    "print(f\"Maximum total rooms: {df_cleaned['total_rooms'].max()}\")\n",
    "print(f\"Average total rooms: {df_cleaned['total_rooms'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Final Data Check\n",
    "\n",
    "Let's verify our cleaned dataset is ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final shape of the dataset\n",
    "print(\"Final dataset summary:\")\n",
    "print(f\"Number of rows: {df_cleaned.shape[0]}\")\n",
    "print(f\"Number of columns: {df_cleaned.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all columns in the cleaned dataset\n",
    "print(\"All columns in cleaned dataset:\")\n",
    "for i, col in enumerate(df_cleaned.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for missing values\n",
    "print(\"Missing values check:\")\n",
    "missing = df_cleaned.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"No missing values in the cleaned dataset!\")\n",
    "else:\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for duplicates\n",
    "print(\"Duplicate check:\")\n",
    "if df_cleaned['id'].duplicated().sum() == 0:\n",
    "    print(\"No duplicate house IDs!\")\n",
    "else:\n",
    "    print(f\"Warning: {df_cleaned['id'].duplicated().sum()} duplicates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final data types\n",
    "print(\"Data types in cleaned dataset:\")\n",
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the cleaned dataset\n",
    "print(\"Preview of cleaned dataset (first 5 rows):\")\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Save the Cleaned Dataset\n",
    "\n",
    "Save the cleaned data to a new CSV file for use in the next phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "output_file = 'cleaned_house_data.csv'\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_file}\")\n",
    "print(f\"Total records: {len(df_cleaned)}\")\n",
    "print(f\"Total columns: {len(df_cleaned.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved file\n",
    "df_verify = pd.read_csv(output_file)\n",
    "print(f\"\\nVerification - File loaded successfully!\")\n",
    "print(f\"Rows: {len(df_verify)}, Columns: {len(df_verify.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data Cleaning\n",
    "\n",
    "### What we did:\n",
    "1. **Loaded** the King County housing dataset (21,613 records)\n",
    "2. **Checked for missing values** - None found\n",
    "3. **Removed duplicate records** - Kept only the latest sale for houses sold multiple times\n",
    "4. **Converted date column** - Changed from string to datetime format\n",
    "5. **Fixed data errors** - Corrected obvious typos (e.g., 33 bedrooms -> 3)\n",
    "6. **Created new features**:\n",
    "   - `sale_year` and `sale_month` from date\n",
    "   - `house_age` (age when sold)\n",
    "   - `renovated` (binary: 0 or 1)\n",
    "   - `price_per_sqft`\n",
    "   - `has_basement` (binary: 0 or 1)\n",
    "   - `total_rooms`\n",
    "7. **Saved** the cleaned data to `cleaned_house_data.csv`\n",
    "\n",
    "### Next Steps:\n",
    "- Phase 2: Exploratory Data Analysis (EDA)\n",
    "- Phase 3: Build Linear Regression model for price prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
